{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Form 5500 Data Downloader\n",
    "\n",
    "Downloads the following files from the DOL EBSA public database for **2009–2023**:\n",
    "\n",
    "| Label | File | What it contains |\n",
    "|---|---|---|\n",
    "| `main_5500` | F_5500 | Plan identity, type, participants, plan year |\n",
    "| `schedule_H` | F_SCH_H | Assets, liabilities, income, expenses, equity allocation |\n",
    "| `schedule_R` | F_SCH_R | Retirement plan info, contribution rates |\n",
    "| `schedule_R1` | F_SCH_R_PART1 | **Employer roster** — EINs of every contributing employer |\n",
    "\n",
    "Files are saved to your **Google Drive** at `MyDrive/form5500/raw/{year}/`\n",
    "\n",
    "**Run cells in order: 1 → 2 → 3 → 4 → 5 → 6**\n",
    "\n",
    "---\n",
    "**Estimated download size:** ~15–25 GB total across all years  \n",
    "**Estimated time:** 45–90 minutes depending on your connection  \n",
    "**Note:** Colab Pro recommended to avoid session timeouts on the full download.\n",
    "For a quick test, run Cell 6 with `years=[2019]` first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 1: Mount Google Drive ───────────────────────────────────────────────\n",
    "# Your files will be saved here and persist after the session ends.\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "print('Drive mounted.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 2: Imports and Configuration ───────────────────────────────────────\n",
    "\n",
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "import csv\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ── Where to save everything ──────────────────────────────────────────────\n",
    "BASE_DIR = Path('/content/drive/MyDrive/form5500/raw')\n",
    "BASE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f'Base directory: {BASE_DIR}')\n",
    "\n",
    "# ── Years to download ─────────────────────────────────────────────────────\n",
    "# DOL switched to the current EFAST2 format in 2009.\n",
    "# 2004-2008 have different URLs and file structures — handled separately below.\n",
    "YEARS = list(range(2009, 2024))  # 2009 through 2023\n",
    "\n",
    "# ── File types ────────────────────────────────────────────────────────────\n",
    "# (label_for_your_reference, DOL_filename_stem)\n",
    "FILE_TYPES = [\n",
    "    ('main_5500',   'F_5500'),          # Main form: plan identity, type, participants\n",
    "    ('schedule_H',  'F_SCH_H'),         # Assets, liabilities, equity allocation\n",
    "    ('schedule_R',  'F_SCH_R'),         # Contribution rates, plan type\n",
    "    ('schedule_R1', 'F_SCH_R_PART1'),   # EMPLOYER ROSTER — the linchpin file\n",
    "]\n",
    "\n",
    "# ── DOL URL base ──────────────────────────────────────────────────────────\n",
    "# All files follow this pattern:\n",
    "# https://askebsa.dol.gov/FOIA Files/{year}/Latest/{stem}_{year}_Latest.zip\n",
    "DOL_BASE = 'https://askebsa.dol.gov/FOIA%20Files'\n",
    "\n",
    "# ── Retry / politeness settings ───────────────────────────────────────────\n",
    "MAX_RETRIES    = 3     # retry failed downloads this many times\n",
    "RETRY_DELAY    = 15   # seconds to wait between retries\n",
    "PAUSE_BETWEEN  = 3    # seconds to pause between each successful download\n",
    "\n",
    "print(f'Will download {len(YEARS)} years × {len(FILE_TYPES)} file types = {len(YEARS)*len(FILE_TYPES)} zip files')\n",
    "print(f'Years: {YEARS[0]}–{YEARS[-1]}')\n",
    "print(f'File types: {[f[0] for f in FILE_TYPES]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 3: Helper Functions ─────────────────────────────────────────────────\n",
    "\n",
    "def build_url(year, file_stem):\n",
    "    \"\"\"Build the DOL download URL for a given year and file stem.\"\"\"\n",
    "    filename = f'{file_stem}_{year}_Latest.zip'\n",
    "    return f'{DOL_BASE}/{year}/Latest/{filename}'\n",
    "\n",
    "\n",
    "def download_file(url, dest_path, label):\n",
    "    \"\"\"\n",
    "    Download url to dest_path with retry logic.\n",
    "    Skips if the file already exists and is > 1KB (avoids re-downloading).\n",
    "    Returns True on success, False on failure.\n",
    "    \"\"\"\n",
    "    dest_path = Path(dest_path)\n",
    "    dest_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Skip if already downloaded and non-trivial size\n",
    "    if dest_path.exists() and dest_path.stat().st_size > 1024:\n",
    "        size_mb = dest_path.stat().st_size / (1024 * 1024)\n",
    "        print(f'  [SKIP]  {label} already exists ({size_mb:.1f} MB)')\n",
    "        return True\n",
    "\n",
    "    for attempt in range(1, MAX_RETRIES + 1):\n",
    "        try:\n",
    "            print(f'  [GET {attempt}/{MAX_RETRIES}] {label}')\n",
    "            response = requests.get(url, timeout=180, stream=True)\n",
    "\n",
    "            if response.status_code == 404:\n",
    "                print(f'  [404]   {label} — file not found at DOL server')\n",
    "                print(f'          URL: {url}')\n",
    "                return False\n",
    "\n",
    "            response.raise_for_status()  # raises on 4xx/5xx\n",
    "\n",
    "            # Stream to disk in 1 MB chunks\n",
    "            bytes_written = 0\n",
    "            with open(dest_path, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=1024 * 1024):\n",
    "                    f.write(chunk)\n",
    "                    bytes_written += len(chunk)\n",
    "\n",
    "            size_mb = bytes_written / (1024 * 1024)\n",
    "            print(f'  [OK]    {label} — {size_mb:.1f} MB')\n",
    "            return True\n",
    "\n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f'  [TIMEOUT] {label} attempt {attempt} timed out')\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f'  [ERR]   {label} attempt {attempt}: {e}')\n",
    "\n",
    "        if attempt < MAX_RETRIES:\n",
    "            print(f'          Retrying in {RETRY_DELAY}s...')\n",
    "            time.sleep(RETRY_DELAY)\n",
    "        else:\n",
    "            print(f'  [FAIL]  {label} — giving up after {MAX_RETRIES} attempts')\n",
    "            return False\n",
    "\n",
    "\n",
    "def unzip_file(zip_path, extract_dir):\n",
    "    \"\"\"\n",
    "    Unzip zip_path into extract_dir.\n",
    "    Skips if CSVs already present. Returns True on success.\n",
    "    \"\"\"\n",
    "    zip_path    = Path(zip_path)\n",
    "    extract_dir = Path(extract_dir)\n",
    "    extract_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Skip if already unzipped\n",
    "    existing = list(extract_dir.glob('*.csv'))\n",
    "    if existing:\n",
    "        print(f'  [SKIP]  Already unzipped ({len(existing)} CSV files in {extract_dir.name}/)')\n",
    "        return True\n",
    "\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "            z.extractall(extract_dir)\n",
    "        csvs = list(extract_dir.glob('*.csv'))\n",
    "        print(f'  [UNZIP] Extracted {len(csvs)} file(s) to {extract_dir.name}/')\n",
    "        return True\n",
    "    except zipfile.BadZipFile:\n",
    "        print(f'  [ERR]   Bad zip — {zip_path.name} is corrupt (deleting for re-download)')\n",
    "        zip_path.unlink(missing_ok=True)\n",
    "        return False\n",
    "\n",
    "\n",
    "def count_rows(csv_dir):\n",
    "    \"\"\"Count rows in the first CSV found in csv_dir. Returns 0 if none.\"\"\"\n",
    "    csvs = list(Path(csv_dir).glob('*.csv'))\n",
    "    if not csvs:\n",
    "        return 0\n",
    "    with open(csvs[0], 'r', encoding='latin-1', errors='replace') as f:\n",
    "        return sum(1 for _ in f) - 1  # subtract header\n",
    "\n",
    "\n",
    "print('Helper functions defined.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 4: Main Download Function ───────────────────────────────────────────\n",
    "\n",
    "def run_downloads(years=YEARS, file_types=FILE_TYPES, unzip=True, validate=True):\n",
    "    \"\"\"\n",
    "    Download, unzip, and validate all Form 5500 files.\n",
    "    Returns a list of result dicts; also writes download_manifest.csv to BASE_DIR.\n",
    "    \"\"\"\n",
    "    print('=' * 65)\n",
    "    print('Form 5500 Downloader')\n",
    "    print(f'Years: {years[0]}–{years[-1]}  |  Files per year: {len(file_types)}')\n",
    "    print(f'Save to: {BASE_DIR}')\n",
    "    print('=' * 65)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for year in years:\n",
    "        year_dir = BASE_DIR / str(year)\n",
    "        year_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        print(f'\\n{\"-\"*55}')\n",
    "        print(f'  YEAR {year}')\n",
    "        print(f'{\"-\"*55}')\n",
    "\n",
    "        for label, stem in file_types:\n",
    "            url      = build_url(year, stem)\n",
    "            zip_path = year_dir / f'{stem}_{year}_Latest.zip'\n",
    "            csv_dir  = year_dir / stem\n",
    "\n",
    "            # Step 1: Download\n",
    "            dl_ok = download_file(url, zip_path, f'{year} / {label}')\n",
    "\n",
    "            row_count = 0\n",
    "            status    = 'DOWNLOAD_FAIL'\n",
    "\n",
    "            if dl_ok:\n",
    "                if unzip:\n",
    "                    # Step 2: Unzip\n",
    "                    uz_ok = unzip_file(zip_path, csv_dir)\n",
    "                    if uz_ok:\n",
    "                        if validate:\n",
    "                            # Step 3: Count rows\n",
    "                            row_count = count_rows(csv_dir)\n",
    "                            if row_count >= 100:\n",
    "                                status = 'OK'\n",
    "                                print(f'  [VAL]   {row_count:,} rows — OK')\n",
    "                            else:\n",
    "                                status = 'WARN_LOW_ROWS'\n",
    "                                print(f'  [WARN]  Only {row_count} rows — check file')\n",
    "                        else:\n",
    "                            status = 'UNZIPPED'\n",
    "                    else:\n",
    "                        status = 'UNZIP_FAIL'\n",
    "                else:\n",
    "                    status = 'DOWNLOADED'\n",
    "\n",
    "            results.append({\n",
    "                'year':      year,\n",
    "                'file_type': label,\n",
    "                'stem':      stem,\n",
    "                'url':       url,\n",
    "                'row_count': row_count,\n",
    "                'status':    status,\n",
    "            })\n",
    "\n",
    "            time.sleep(PAUSE_BETWEEN)\n",
    "\n",
    "    # Write manifest\n",
    "    manifest_path = BASE_DIR / 'download_manifest.csv'\n",
    "    with open(manifest_path, 'w', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=results[0].keys())\n",
    "        writer.writeheader()\n",
    "        writer.writerows(results)\n",
    "\n",
    "    # Summary\n",
    "    ok   = sum(1 for r in results if r['status'] == 'OK')\n",
    "    skip = sum(1 for r in results if 'SKIP' in r['status'])\n",
    "    fail = sum(1 for r in results if 'FAIL' in r['status'])\n",
    "    warn = sum(1 for r in results if 'WARN' in r['status'])\n",
    "\n",
    "    print(f'\\n{\"=\"*65}')\n",
    "    print(f'COMPLETE — manifest saved to {manifest_path}')\n",
    "    print(f'  ✓  OK:       {ok}')\n",
    "    print(f'  ↷  Skipped:  {skip}  (already existed)')\n",
    "    print(f'  ⚠  Warnings: {warn}  (low row count — inspect)')\n",
    "    print(f'  ✗  Failed:   {fail}  (see manifest for URLs)')\n",
    "    print(f'{\"=\"*65}')\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "print('run_downloads() defined.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 5: TEST FIRST — Download one year only ───────────────────────────────\n",
    "# Run this before the full download to confirm everything works.\n",
    "# 2019 is a good test year: post-GFC, pre-COVID, clean data.\n",
    "\n",
    "test_results = run_downloads(\n",
    "    years=[2019],\n",
    "    file_types=FILE_TYPES\n",
    ")\n",
    "\n",
    "# If all 4 files show OK, proceed to Cell 6 for the full download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 6: FULL DOWNLOAD — All years 2009-2023 ───────────────────────────────\n",
    "# Only run after Cell 5 confirms the test year works.\n",
    "# This will take 45-90 minutes. Already-downloaded files are skipped.\n",
    "# Safe to re-run if interrupted — it picks up where it left off.\n",
    "\n",
    "results = run_downloads(\n",
    "    years=YEARS,\n",
    "    file_types=FILE_TYPES\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 7: Review Results ────────────────────────────────────────────────────\n",
    "\n",
    "manifest = pd.read_csv(BASE_DIR / 'download_manifest.csv')\n",
    "\n",
    "print('Row counts by year and file type:')\n",
    "print('(These are record counts in the CSV — gives a sense of scale)')\n",
    "print()\n",
    "pivot = manifest.pivot_table(\n",
    "    index='year',\n",
    "    columns='file_type',\n",
    "    values='row_count',\n",
    "    aggfunc='sum'\n",
    ")\n",
    "print(pivot.to_string())\n",
    "\n",
    "# Anything that needs attention:\n",
    "problems = manifest[~manifest['status'].isin(['OK', 'SKIP'])]\n",
    "if len(problems) > 0:\n",
    "    print(f'\\n⚠  {len(problems)} files need attention:')\n",
    "    print(problems[['year', 'file_type', 'status', 'url']].to_string())\n",
    "else:\n",
    "    print('\\n✓ All files OK.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 8: Sanity Check — Peek at Files ────────────────────────────────────\n",
    "# Open a sample year of each file type and print shape + column names.\n",
    "# This confirms the CSVs are readable and have the expected structure.\n",
    "\n",
    "SAMPLE_YEAR = 2019\n",
    "\n",
    "for label, stem in FILE_TYPES:\n",
    "    csv_dir = BASE_DIR / str(SAMPLE_YEAR) / stem\n",
    "    csvs = list(csv_dir.glob('*.csv'))\n",
    "    if not csvs:\n",
    "        print(f'\\n{stem}: NO CSV FOUND in {csv_dir}')\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(csvs[0], nrows=3, encoding='latin-1', low_memory=False)\n",
    "    print(f'\\n{\"-\"*55}')\n",
    "    print(f'{label} ({stem}) — {csvs[0].name}')\n",
    "    print(f'  Columns: {len(df.columns)}')\n",
    "    print(f'  First 5 columns: {list(df.columns[:5])}')\n",
    "    # Key columns to confirm present:\n",
    "    key_cols = {\n",
    "        'main_5500':   ['ACK_ID', 'PLAN_NAME', 'SPONS_DFE_EIN', 'PLAN_NUM'],\n",
    "        'schedule_H':  ['ACK_ID', 'SCH_H_TOT_ASSETS_BOY_AMT', 'SCH_H_TOT_ASSETS_EOY_AMT'],\n",
    "        'schedule_R':  ['ACK_ID', 'SCH_R_TOT_PARTCP_BOY_CNT'],\n",
    "        'schedule_R1': ['ACK_ID', 'SCH_R_CONTRIBING_EMPL_EIN'],\n",
    "    }\n",
    "    if label in key_cols:\n",
    "        found    = [c for c in key_cols[label] if c in df.columns]\n",
    "        missing  = [c for c in key_cols[label] if c not in df.columns]\n",
    "        print(f'  Key columns present: {found}')\n",
    "        if missing:\n",
    "            print(f'  ⚠ Missing expected columns: {missing}')\n",
    "            print(f'    (column names may differ by year — check layout file)')\n",
    "\n",
    "print(f'\\n{\"-\"*55}')\n",
    "print('Sanity check complete.')\n",
    "print(f'Files are at: {BASE_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 9: OPTIONAL — 2004-2008 Early Years ─────────────────────────────────\n",
    "# The 2004-2008 data uses a different URL structure and older file format.\n",
    "# These years are useful for pre-period analysis but have worse data quality.\n",
    "# Download only if you need the full 2004-2023 panel.\n",
    "\n",
    "EARLY_YEARS = list(range(2004, 2009))  # 2004-2008\n",
    "\n",
    "# Early years URL: https://www.dol.gov/sites/dolgov/files/ebsa/researchers/\n",
    "#                  analysis/form-5500/{year}-form-5500-datasets.zip\n",
    "# These are single zip files per year containing all schedules together.\n",
    "\n",
    "EARLY_DOL_BASE = 'https://www.dol.gov/sites/dolgov/files/ebsa/researchers/analysis/form-5500'\n",
    "\n",
    "def download_early_year(year):\n",
    "    \"\"\"Download the combined zip for 2004-2008 years.\"\"\"\n",
    "    url       = f'{EARLY_DOL_BASE}/{year}-form-5500-datasets.zip'\n",
    "    year_dir  = BASE_DIR / str(year)\n",
    "    year_dir.mkdir(parents=True, exist_ok=True)\n",
    "    zip_path  = year_dir / f'form5500_{year}_all.zip'\n",
    "\n",
    "    print(f'\\nYear {year}')\n",
    "    success = download_file(url, zip_path, f'{year} early-format')\n",
    "    if success:\n",
    "        unzip_file(zip_path, year_dir)\n",
    "    return success\n",
    "\n",
    "# Uncomment to run:\n",
    "# for y in EARLY_YEARS:\n",
    "#     download_early_year(y)\n",
    "#     time.sleep(PAUSE_BETWEEN)\n",
    "\n",
    "print('Early-years download function defined (not yet run).')\n",
    "print('Uncomment the loop above when ready.')"
   ]
  }
 ]
}
